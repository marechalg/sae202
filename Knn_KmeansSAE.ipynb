{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN :  K-Nearest Neighbors Algorithme (algorthme des K plus proches voisins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme KNN (k-nearest neighbors) est un algorithme d'apprentissage automatique supervisé simple et facile à mettre en œuvre, qui peut être utilisé pour résoudre les problèmes de classification et de régression (qu'est ce qu'une régréssion ? une classification ?).  \n",
    "Un algorithme d'apprentissage automatique supervisé est un algorithme qui s'appuie sur des données d'entrée étiquetées (des couples données-étiquettes).\n",
    "On dispose donc de données d'apprentissage et lors de la prédiction, lorsqu'on rencontre une nouvelle donnée (de test) à prédire, on cherche les K instances d'entraînement les plus proches de cette nouvelle donnée.  \n",
    "On attribue ensuite  à cette donnée la classe (étiquette) la plus courante parmi ces K instances d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ainsi besoin d'une fonction mathématique pour mesurer la distance séparant nos objets (données). On supposera que nos données sont des vecteurs de $\\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **distance de Minkowski** entre deux points $A = (x_1, x_2, \\dots, x_n)$ et $B = (y_1, y_2, \\dots, y_n) $ dans un espace à $n$ dimensions (pour nous $\\mathbb{R}^n$) est définie par :\n",
    "$$\n",
    "d_p(A, B) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{\\frac{1}{p}}, \\quad p \\geq 1\n",
    "$$\n",
    "\n",
    "Cas particuliers :\n",
    "- Pour $p = 1$, on obtient la **distance de Manhattan** :\n",
    "  $$ d_1(A, B) = \\sum_{i=1}^{n} |x_i - y_i| $$\n",
    "- Pour $p = 2$, on obtient la **distance Euclidienne** :\n",
    "$$ d_2(A, B) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "- Pour $p \\to \\infty$, on obtient la **distance de Chebyshev** (ou $l_\\infty$) :\n",
    "$$ d_{\\infty}(A, B) = \\max_{1 \\leq i \\leq n} |x_i - y_i| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles propriétés doit vérifier une fonction (espaces de départ et d'arrivée ?) pour être une distance ? Pour $p=1$ vérifiez que ces propriétés sont vérifiées.    \n",
    "Pour \"voir\" les différences entre ces distances, au moins pour les trois cas particuliers, on pourra tracer dans $\\mathbb{R}^2$ la boule unité correspondante (les points du plan qui sont à une distance inférieure ou égale à 1 de l'origine).  \n",
    "On pourra également se poser la question du choix de la distance...  \n",
    "On choisira également $k$ (le nombre de voisins les plus proches à retenir). Là aussi on se posera la question du choix de $k$ et des conséquences (s'il est trop petit, trop grand ?).   \n",
    "On peut alors mettre en oeuvre l'algorithme en suivant les étapes ci-dessous :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Charger les données\n",
    "2. Pour chaque donnée\n",
    "   - Calculer la distance entre la donnée en question et l'enregistrement courant.\n",
    "   - Ajouter la distance et l'indice de l'enregistrement dans une collection.\n",
    "3. Trier la collection par distances croissantes.\n",
    "4. Choisir les $k$ premières entrées de la collection\n",
    "5. Obtenir les étiquettes de ces entrées\n",
    "6.   - S'il s'agit d'une régression, retourner la valeur moyenne des étiquettes\n",
    "     - S'il s'agit d'un classification, retourner la valeur la plus courante parmi les étiquettes des $k$ voisins retenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra, dans un premier temps, pour tester utiliser comme données d'apprentissage des points de $\\mathbb{N}^2$ étiquettés par des lettres de l'alphabet (on pourra se limiter un petit nombre pour les tests, et même les représenter graphiquement).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se posera également la question de la normalisation des données (de gros écarts entre les valeurs, par exmple la taille en mètre et le poids en kilogramme).\n",
    "De même que se passe t-il si nos données sont de grande dimension (que se passe t'il pour la distance des données à notre donnée de test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment apprécier la qualité de notre classification (régréssion) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'en est-il de la compléxité de cet algorithme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On testera également cet algorithme en utilisant la bibliothèque libre  **Scikit-learn (sklearn)**    destinée à l'apprentissage automatique.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_openml, load_digits\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml, load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "# Récupère les images des chiffres et les labels correspondant\n",
    "mnist = fetch_openml(\"mnist_784\") \n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "\n",
    "x = np.array(mnist.data)\n",
    "y = np.array(mnist.target)\n",
    "\n",
    "# On peut également mélanger le tout pour ne pas tester toujours le même chiffre \n",
    "#rd = np.random.permutation(x.shape[0])\n",
    "#x = x[rd]\n",
    "#y = y[rd]\n",
    "\n",
    "nbr = x[12]\n",
    "# nbr correspond aux pixels de  l'image 28x28 = 784 \n",
    "nbr_image = nbr.reshape(28, 28)\n",
    "plt.imshow(nbr_image, cmap=matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "# On peut vérifier que le label y[12] correspond bien au nombre affiché"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means (K-moyennes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donnés un ensemble de points et un entier $k$, le problème est de trouver une **partition** (des clusters ou grappes) de ces points en $k$ groupes minimisant la variance à l'intérieur de chaque groupe. Les données sont cette fois non étiquetées.  \n",
    "Plus précisément, il s'agit de minimiser la somme des carrés des distances de chaque point à la moyenne des points de son groupe.  \n",
    "On choisira dans un premier temps $k$ au hasard et on se donne un jeu de données (une matrice où chaque ligne représente une donnée).  \n",
    "Ensuite, Choisir aléatoirement $k$ points,  ces points sont les centres des clusters (appelés aussi centroïd), puis on répète :  \n",
    "1. Affecter chaque point (ligne de  la matrice de données) au groupe dont il est le plus proche (de son centre).  \n",
    "2. Recalculer le centre de chaque cluster et modifier le centroide. On prendra simplement la moyenne des points du cluster.\n",
    "\n",
    "On peut soit déterminer (fixer) à l'avance le nombre d'itération, soit considérer qu'il y a convergence lorsque les centroides n'évoluent plus significativement (on peut également pour éviter les mauvaises surprises mixer les deux conditions).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans le cas de l'algorithme **Knn**, on se posera la question du choix de $k$ (méthode Elbow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettre en oeuvre cet algorithme et le tester en utilisant les **scikit-learn** (comme précédement). On pourra utiliser comme données  les images  correspondant aux entiers (qu'on supposera ici non étiquetées)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
