{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN :  K-Nearest Neighbors Algorithme (algorthme des K plus proches voisins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme KNN (k-nearest neighbors) est un algorithme d'apprentissage automatique supervisé simple et facile à mettre en œuvre, qui peut être utilisé pour résoudre les problèmes de classification et de régression (qu'est ce qu'une régréssion ? une classification ?).  \n",
    "Un algorithme d'apprentissage automatique supervisé est un algorithme qui s'appuie sur des données d'entrée étiquetées (des couples données-étiquettes).\n",
    "On dispose donc de données d'apprentissage et lors de la prédiction, lorsqu'on rencontre une nouvelle donnée (de test) à prédire, on cherche les K instances d'entraînement les plus proches de cette nouvelle donnée.  \n",
    "On attribue ensuite  à cette donnée la classe (étiquette) la plus courante parmi ces K instances d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ainsi besoin d'une fonction mathématique pour mesurer la distance séparant nos objets (données). On supposera que nos données sont des vecteurs de $\\mathbb{R}^n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **distance de Minkowski** entre deux points $A = (x_1, x_2, \\dots, x_n)$ et $B = (y_1, y_2, \\dots, y_n) $ dans un espace à $n$ dimensions (pour nous $\\mathbb{R}^n$) est définie par :\n",
    "$$\n",
    "d_p(A, B) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{\\frac{1}{p}}, \\quad p \\geq 1\n",
    "$$\n",
    "\n",
    "Cas particuliers :\n",
    "- Pour $p = 1$, on obtient la **distance de Manhattan** :\n",
    "  $$ d_1(A, B) = \\sum_{i=1}^{n} |x_i - y_i| $$\n",
    "- Pour $p = 2$, on obtient la **distance Euclidienne** :\n",
    "$$ d_2(A, B) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $$\n",
    "- Pour $p \\to \\infty$, on obtient la **distance de Chebyshev** (ou $l_\\infty$) :\n",
    "$$ d_{\\infty}(A, B) = \\max_{1 \\leq i \\leq n} |x_i - y_i| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelles propriétés doit vérifier une fonction (espaces de départ et d'arrivée ?) pour être une distance ? Pour $p=1$ vérifiez que ces propriétés sont vérifiées.    \n",
    "Pour \"voir\" les différences entre ces distances, au moins pour les trois cas particuliers, on pourra tracer dans $\\mathbb{R}^2$ la boule unité correspondante (les points du plan qui sont à une distance inférieure ou égale à 1 de l'origine).  \n",
    "On pourra également se poser la question du choix de la distance...  \n",
    "On choisira également $k$ (le nombre de voisins les plus proches à retenir). Là aussi on se posera la question du choix de $k$ et des conséquences (s'il est trop petit, trop grand ?).   \n",
    "On peut alors mettre en oeuvre l'algorithme en suivant les étapes ci-dessous :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Charger les données\n",
    "2. Pour chaque donnée\n",
    "   - Calculer la distance entre la donnée en question et l'enregistrement courant.\n",
    "   - Ajouter la distance et l'indice de l'enregistrement dans une collection.\n",
    "3. Trier la collection par distances croissantes.\n",
    "4. Choisir les $k$ premières entrées de la collection\n",
    "5. Obtenir les étiquettes de ces entrées\n",
    "6.   - S'il s'agit d'une régression, retourner la valeur moyenne des étiquettes\n",
    "     - S'il s'agit d'un classification, retourner la valeur la plus courante parmi les étiquettes des $k$ voisins retenus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra, dans un premier temps, pour tester utiliser comme données d'apprentissage des points de $\\mathbb{N}^2$ étiquettés par des lettres de l'alphabet (on pourra se limiter un petit nombre pour les tests, et même les représenter graphiquement).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se posera également la question de la normalisation des données (de gros écarts entre les valeurs, par exmple la taille en mètre et le poids en kilogramme).\n",
    "De même que se passe t-il si nos données sont de grande dimension (que se passe t'il pour la distance des données à notre donnée de test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment apprécier la qualité de notre classification (régréssion) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettre les valeurs dans un tableau de données dans un ordre définis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qu'en est-il de la compléxité de cet algorithme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour notre algorythme :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(self, col, k):\n",
    "        res = 0\n",
    "        for val in col:\n",
    "            res += col[val]\n",
    "        return res / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a une compléxité de O(n) ou n est la taille de \"col\"\n",
    "car la compléxité de la boucle \"for val in col\" est dominante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On testera également cet algorithme en utilisant la bibliothèque libre  **Scikit-learn (sklearn)**    destinée à l'apprentissage automatique.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_optional_dependencies.py:42\u001b[39m, in \u001b[36mcheck_pandas_support\u001b[39m\u001b[34m(caller_name)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pandas\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:1059\u001b[39m, in \u001b[36mfetch_openml\u001b[39m\u001b[34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m     \u001b[43mcheck_pandas_support\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m`fetch_openml`\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_optional_dependencies.py:46\u001b[39m, in \u001b[36mcheck_pandas_support\u001b[39m\u001b[34m(caller_name)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m requires pandas.\u001b[39m\u001b[33m\"\u001b[39m.format(caller_name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: `fetch_openml` requires pandas.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Récupère les images des chiffres et les labels correspondant\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m mnist = \u001b[43mfetch_openml\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmnist_784\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m      9\u001b[39m mnist.target = mnist.target.astype(np.int8)\n\u001b[32m     11\u001b[39m x = np.array(mnist.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\datasets\\_openml.py:1072\u001b[39m, in \u001b[36mfetch_openml\u001b[39m\u001b[34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser, read_csv_kwargs)\u001b[39m\n\u001b[32m   1067\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1068\u001b[39m             err_msg = (\n\u001b[32m   1069\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing `parser=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparser\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m` with dense data requires pandas to be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1070\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstalled. Alternatively, explicitly set `parser=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mliac-arff\u001b[39m\u001b[33m'\u001b[39m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1071\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(err_msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_sparse:\n\u001b[32m   1075\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m as_frame:\n",
      "\u001b[31mImportError\u001b[39m: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`."
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml, load_digits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random\n",
    "\n",
    "# Récupère les images des chiffres et les labels correspondant\n",
    "mnist = fetch_openml(\"mnist_784\") \n",
    "mnist.target = mnist.target.astype(np.int8)\n",
    "\n",
    "x = np.array(mnist.data)\n",
    "y = np.array(mnist.target)\n",
    "\n",
    "# On peut également mélanger le tout pour ne pas tester toujours le même chiffre \n",
    "#rd = np.random.permutation(x.shape[0])\n",
    "#x = x[rd]\n",
    "#y = y[rd]\n",
    "\n",
    "nbr = x[12]\n",
    "# nbr correspond aux pixels de  l'image 28x28 = 784 \n",
    "nbr_image = nbr.reshape(28, 28)\n",
    "plt.imshow(nbr_image, cmap=matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "# On peut vérifier que le label y[12] correspond bien au nombre affiché"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means (K-moyennes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant donnés un ensemble de points et un entier $k$, le problème est de trouver une **partition** (des clusters ou grappes) de ces points en $k$ groupes minimisant la variance à l'intérieur de chaque groupe. Les données sont cette fois non étiquetées.  \n",
    "Plus précisément, il s'agit de minimiser la somme des carrés des distances de chaque point à la moyenne des points de son groupe.  \n",
    "On choisira dans un premier temps $k$ au hasard et on se donne un jeu de données (une matrice où chaque ligne représente une donnée).  \n",
    "Ensuite, Choisir aléatoirement $k$ points,  ces points sont les centres des clusters (appelés aussi centroïd), puis on répète :  \n",
    "1. Affecter chaque point (ligne de  la matrice de données) au groupe dont il est le plus proche (de son centre).  \n",
    "2. Recalculer le centre de chaque cluster et modifier le centroide. On prendra simplement la moyenne des points du cluster.\n",
    "\n",
    "On peut soit déterminer (fixer) à l'avance le nombre d'itération, soit considérer qu'il y a convergence lorsque les centroides n'évoluent plus significativement (on peut également pour éviter les mauvaises surprises mixer les deux conditions).\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans le cas de l'algorithme **Knn**, on se posera la question du choix de $k$ (méthode Elbow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mettre en oeuvre cet algorithme et le tester en utilisant les **scikit-learn** (comme précédement). On pourra utiliser comme données  les images  correspondant aux entiers (qu'on supposera ici non étiquetées)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
